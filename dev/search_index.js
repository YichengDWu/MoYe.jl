var documenterSearchIndex = {"docs":
[{"location":"manual/datamovement/gs/#Matrix-Transpose-Tutorial","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"","category":"section"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"This tutorial illustrates the process copying data between global memory and shared memory using MoYe. ","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"In this tutorial, we will use the following configuration:","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Array size: 2048 x 2048\nBlock size: 32 x 32\nThread size: 32 x 8","category":"page"},{"location":"manual/datamovement/gs/#Copy-Kernel","page":"Matrix Transpose Tutorial","title":"Copy Kernel","text":"","category":"section"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"We start with a copy kernel.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"using MoYe, Test, CUDA\n\nfunction copy_kernel(dest, src, smemlayout, blocklayout, threadlayout)\n    moye_smem = MoYeSharedArray(eltype(dest), smemlayout) \n\n    moye_dest = MoYeArray(dest)\n    moye_src = MoYeArray(src)\n\n    bM = size(blocklayout, 1)\n    bN = size(blocklayout, 2)\n\n    blocktile_dest = @tile moye_dest (bM, bN) (blockIdx().x, blockIdx().y)\n    blocktile_src  = @tile moye_src  (bM, bN) (blockIdx().x, blockIdx().y)\n\n    threadtile_dest = @parallelize blocktile_dest threadlayout threadIdx().x\n    threadtile_src  = @parallelize blocktile_src  threadlayout threadIdx().x\n    threadtile_smem = @parallelize moye_smem      threadlayout threadIdx().x\n\n    for i in eachindex(threadtile_smem)\n        threadtile_smem[i] = threadtile_src[i]\n    end\n    \n    for i in eachindex(threadtile_dest)\n        threadtile_dest[i] = threadtile_smem[i]\n    end\n    return nothing\nend\n\nfunction test_copy_async(M, N)\n    a = CUDA.rand(Float32, M, N)\n    b = CUDA.rand(Float32, M, N)\n\n    blocklayout = @Layout (32, 32) # 32 * 32 elements in a block\n    smemlayout = @Layout (32, 32)  # 32 * 32 elements in shared memory\n    threadlayout = @Layout (32, 8) # 32 * 8 threads in a block\n\n    bM = size(blocklayout, 1)\n    bN = size(blocklayout, 2)\n\n    blocks = (cld(M, bM), cld(N, bN))\n    threads = Int(size(threadlayout))\n\n    @cuda blocks=blocks threads=threads copy_kernel(a, b, smemlayout, blocklayout, threadlayout)\n    CUDA.synchronize()\n    @test a == b\nend\n\ntest_copy_async(2048, 2048)","category":"page"},{"location":"manual/datamovement/gs/#Code-Explanation","page":"Matrix Transpose Tutorial","title":"Code Explanation","text":"","category":"section"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"The device function follows these steps:","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Allocate shared memory using MoYeSharedArray with a static layout.\nWrap the destination and source arrays with dynamic layouts.\nGet the size of each block in the grid (bM and bN).\nCreate local tiles for the destination and source arrays using @tile.\nPartition the local tiles into thread tiles using @parallelize.\nCopy data from the source thread tile to the shared memory thread tile.\nSynchronize threads.\nCopy data back from the shared memory thread tile to the destination thread tile.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"The host function tests the copy_kernel function with the following steps:","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Define the dimensions M and N for the source and destination arrays.\nCreate random GPU arrays a and b with the specified dimensions using CUDA.rand.\nDefine the block and thread layouts using @Layout for creating static layouts.\nCalculate the number of blocks in the grid using cld. Here we assume the divisibility.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"A few things to notice here:","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"@tile means that all of our blocks cover the entire array.\nEach block contains 32 x 32 elements of the original array, but we have 32 x 8 threads per block, which means that each thread processes 4 elements. The code","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"@parallelize blocktile_dest threadlayout threadIdx().x","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"returns the set of elements that the thread corresponding to threadIdx().x is processing, which in this case is an array of length 4.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Once we have completed all the tiling, we just perform computations as if we were dealing with a regular array:","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"for i in eachindex(threadtile_smem)\n    threadtile_smem[i] = threadtile_src[i]\nend","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"You need not concern yourself with index bookkeeping, it is implicitly handled by the layout; instead, concentrate on the computation aspect, as it is a fundamental objective of MoYe.jl.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Additionally, you can use the copyto! function for static MoYeArray with two key feature: copying from global memory to shared memory automatically calls cp.async (Requires sm_80 or higher), and automatic vectorization when possible.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Here is how it would look like using copyto!.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"function copy_kernel(dest, src, smemlayout, blocklayout, threadlayout)\n    moye_smem = MoYeSharedArray(eltype(dest), smemlayout) \n\n    moye_dest = MoYeArray(dest)\n    moye_src = MoYeArray(src)\n\n    bM = size(blocklayout, 1)\n    bN = size(blocklayout, 2)\n\n    blocktile_dest = @tile moye_dest (bM, bN) (blockIdx().x, blockIdx().y)\n    blocktile_src  = @tile moye_src  (bM, bN) (blockIdx().x, blockIdx().y)\n\n    threadtile_dest = @parallelize blocktile_dest threadlayout threadIdx().x\n    threadtile_src  = @parallelize blocktile_src  threadlayout threadIdx().x\n    threadtile_smem = @parallelize moye_smem      threadlayout threadIdx().x\n\n    copyto!(threadtile_smem, threadtile_src)\n    cp_async_wait()\n    copyto!(threadtile_dest, threadtile_smem)\n\n    return nothing\nend","category":"page"},{"location":"manual/datamovement/gs/#Padding-Shared-Memory","page":"Matrix Transpose Tutorial","title":"Padding Shared Memory","text":"","category":"section"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Note that in the above code, the layout of the shared memory is the same as the block layout. However, we often need to pad the shared array to avoid bank conflicts. We just need to change one line of code:","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"smemlayout = @Layout (32, 32) (1, 31)  # pad one row","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Also note that our kernel will recompile for different static layout parameters.","category":"page"},{"location":"manual/datamovement/gs/#Transpose-kernel","page":"Matrix Transpose Tutorial","title":"Transpose kernel","text":"","category":"section"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"Now we turn to the transpose kernel.","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"function transpose_kernel(dest, src, smemlayout, blocklayout, threadlayout)\n    moye_smem = MoYeSharedArray(eltype(dest), smemlayout) \n\n    moye_src = MoYeArray(src)\n    moye_dest = MoYeArray(dest)\n\n    bM = size(blocklayout, 1)\n    bN = size(blocklayout, 2)\n\n    blocktile_src  = @tile moye_src  (bM, bN) (blockIdx().x, blockIdx().y)\n    blocktile_dest = @tile moye_dest (bN, bM) (blockIdx().y, blockIdx().x)\n\n    threadtile_dest = @parallelize blocktile_dest threadlayout threadIdx().x\n    threadtile_src  = @parallelize blocktile_src  threadlayout threadIdx().x\n    threadtile_smem = @parallelize moye_smem      threadlayout threadIdx().x\n\n    copyto!(threadtile_smem, threadtile_src)\n    cp_async_wait()\n    sync_threads()\n\n    moye_smem′ = MoYe.transpose(moye_smem)\n    threadtile_smem′ = @parallelize moye_smem′ threadlayout threadIdx().x\n\n    copyto!(threadtile_dest, threadtile_smem′)\n    return nothing\nend\n\n\nfunction test_transpose(M, N)\n    a = CUDA.rand(Float32, M, N)\n    b = CUDA.rand(Float32, N, M)\n\n    blocklayout = @Layout (32, 32)\n    smemlayout = @Layout (32, 32) (1, 33)\n    threadlayout = @Layout (32, 8)\n\n    bM = size(blocklayout, 1)\n    bN = size(blocklayout, 2)\n\n    blocks = (cld(M, bM), cld(N, bN))\n    threads = Int(size(threadlayout))\n\n    @cuda blocks=blocks threads=threads transpose_kernel(a, b, smemlayout, blocklayout, threadlayout)\n    CUDA.synchronize()\n    @test a == transpose(b)\nend\n\ntest_transpose(2048, 2048)","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"It is almost identical to the copy kernel， but we would need to transpose the shared memory by simply transposing its layout","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"    moye_smem′ = MoYe.transpose(moye_smem)","category":"page"},{"location":"manual/datamovement/gs/","page":"Matrix Transpose Tutorial","title":"Matrix Transpose Tutorial","text":"and then compute the new thread tiles. Note that each thread would work on different elements now so we need to call sync_threads().","category":"page"},{"location":"manual/array/#MoYeArray","page":"Array","title":"MoYeArray","text":"","category":"section"},{"location":"manual/array/","page":"Array","title":"Array","text":"MoYeArray befinits from the Layout to be able to create all kinds of special arrays. For example, we can create a FillArray-like array","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"using MoYe\nMoYeArray{Float64}(one, @Layout((3,4), (0, 0)))\nans.engine","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"As you can see the array only contains one element. Under the hood, the physical length of that array is calculated by cosize:","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"cosize(@Layout((3,4), (0, 0)))","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"The underlying implementation of MoYeArray determines that linear indexing is actually periodic.","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"function f()\n    B = MoYeArray([1,2,3], @Layout((3,), (1,)))\n    @show @inbounds B[4], B[5], B[6], B[7]\nend\nf();","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"We can also easily create a so-called BlockArray:","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"data = collect(1:48);\nB=MoYeArray(data, @Layout(((2,3), (2,4)), ((1, 16), (2, 4))))","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"Here we created a 2x3 block array with 2x4 blocks. The first mode is the index of the block, the second mode is the index within the block.","category":"page"},{"location":"manual/array/#Slicing","page":"Array","title":"Slicing","text":"","category":"section"},{"location":"manual/array/","page":"Array","title":"Array","text":"It is required to use the  syntax view(a, ids...) or@view a[ids...]`, depenting on your tast. ","category":"page"},{"location":"manual/array/","page":"Array","title":"Array","text":"data = [i for i in 1:164];\na = MoYeArray(data, ((_3, 2), (2, _5, _2)), ((4,1), (_2, 13, 100)))\nb = @view a[2, :]","category":"page"},{"location":"api/tiling/#Index","page":"Tiling","title":"Index","text":"","category":"section"},{"location":"api/tiling/","page":"Tiling","title":"Tiling","text":"Pages = [\"tiling.md\"]","category":"page"},{"location":"api/tiling/#MoYe.@tile","page":"Tiling","title":"MoYe.@tile","text":"@tile x::MoYeArray threadgroup_shape::Tile threadgroup_coord::Tuple\n\nPartition x with threadgroup_shape. Return the view of the entries of x that the thread group at threadgroup_coord will work on.\n\nExamples\n\njulia> a = MoYeArray(pointer([i for i in 1:48]), @Layout((6,8)))\n6×8 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{6}, Static.StaticInt{8}}, Tuple{Static.StaticInt{1}, Static.StaticInt{6}}}} with indices _1:_6×_1:_8:\n 1   7  13  19  25  31  37  43\n 2   8  14  20  26  32  38  44\n 3   9  15  21  27  33  39  45\n 4  10  16  22  28  34  40  46\n 5  11  17  23  29  35  41  47\n 6  12  18  24  30  36  42  48\n\njulia> @tile a (_2, _2) (1, 1)\n2×2 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{StaticInt{2}, StaticInt{2}}, Tuple{StaticInt{1}, StaticInt{6}}}}:\n 1  7\n 2  8\n\n\n\n\n\n\n","category":"macro"},{"location":"api/tiling/#MoYe.@parallelize","page":"Tiling","title":"MoYe.@parallelize","text":"@parallelize x::MoYeArray threadgroup_layout::Layout thread_idx::Int\n\nPartition x with size(threadgroup_layout) threads, and return the view of the entries that the thread at thread_idx will work on.\n\nExamples\n\nSay we have a MoYeArray x of shape (6, 8) and 4 threads of shape (2, 2). We would like to  partition x with the 4 threads and get a view of the entries that the first thread will work on. We can do this by calling @parallelize(x, (2, 2), 1).\n\njulia> a = MoYeArray(pointer([i for i in 1:48]), @Layout((6,8)))\n6×8 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{6}, Static.StaticInt{8}}, Tuple{Static.StaticInt{1}, Static.StaticInt{6}}}}:\n 1   7  13  19  25  31  37  43\n 2   8  14  20  26  32  38  44\n 3   9  15  21  27  33  39  45\n 4  10  16  22  28  34  40  46\n 5  11  17  23  29  35  41  47\n 6  12  18  24  30  36  42  48\n\njulia> @parallelize a (_2, _2) (1, 1)\n3×4 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{3}, Static.StaticInt{4}}, Tuple{Static.StaticInt{2}, Static.StaticInt{12}}}}:\n 1  13  25  37\n 3  15  27  39\n 5  17  29  41\n\nYou can also pass in a thread layout and a thread id to get the tile:\n\njulia> @parallelize a @Layout((2,2), (1, 2)) 2\n3×4 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{StaticInt{3}, StaticInt{4}}, Tuple{StaticInt{2}, StaticInt{12}}}}:\n 2  14  26  38\n 4  16  28  40\n 6  18  30  42\n\njulia> @parallelize a @Layout((2,2), (2, 1)) 2\n3×4 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{StaticInt{3}, StaticInt{4}}, Tuple{StaticInt{2}, StaticInt{12}}}}:\n  7  19  31  43\n  9  21  33  45\n 11  23  35  47\n\n\n\n\n\n","category":"macro"},{"location":"manual/async/#Memcpy-Async","page":"Memcpy Async","title":"Memcpy Async","text":"","category":"section"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"With the NVIDIA Ampere architecture, you can asynchronously copy data between GPU global memory and shared memory and not tie up threads to shepherd data movement.","category":"page"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"To utilize this feature, we simply change the TiledCopy to the following ","category":"page"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"copy_A = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{TA}, TA}(),\n                                @Layout((32, 8)),\n                                @Layout((1, 1)))\ncopy_B = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{TB}, TB}(),\n                                    @Layout((32, 8)),\n                                    @Layout((1, 1)))","category":"page"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"The updated kernel function.","category":"page"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"function matmul_kernel(A, sA_layout, copy_A,\n                       B, sB_layout, copy_B,\n                       C, mma_C)\n    sA = MoYeSharedArray(eltype(A), sA_layout)\n    sB = MoYeSharedArray(eltype(B), sB_layout)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # copy partition\n    thr_copy_a = get_slice(copy_A, threadIdx().x)      \n    tAgA = partition_S(thr_copy_a, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(thr_copy_a, sA)                 # (CPY, CPY_M, CPY_K)\n\n    thr_copy_b = get_slice(copy_B, threadIdx().x)\n    tBgB = partition_S(thr_copy_b, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(thr_copy_b, sB)                 # (CPY, CPY_N, CPY_K)\n\n    # mma partition\n    thr_mma = get_slice(mma_C, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                    # (MMA, MMA_M, MMA_K)\n    tCsB = partition_B(thr_mma, sB)                    # (MMA, MMA_M, MMA_K)\n    tCgC = partition_C(thr_mma, gC)                    # (MMA, MMA_M, MMA_N)\n\n    # accumulator\n    tCrC = make_fragment_C(thr_mma, tCgC)\n    zeros!(tCrC)\n\n    for k in axes(tAgA, 4)\n        copyto!(copy_A, tAsA, view(tAgA, :, :, :, k))\n        copyto!(copy_B, tBsB, view(tBgB, :, :, :, k))\n        \n        cp_async_wait()\n\n        @gc_preserve gemm!(mma_C, tCrC, tCsA, tCsB, tCrC)\n        sync_threads()\n    end\n\n\n    copyto!(tCgC, tCrC)\n    return nothing\nend\n\nfunction matmul(A, B, C)\n    bM = _128\n    bN = _128\n    bK = _8\n    \n    sA_layout = make_layout((bM, bK), (_1, bM + _1))\n    sB_layout = make_layout((bN, bK), (_1, bN + _1))\n\n    TA = eltype(A)\n    TB = eltype(B)\n    TC = eltype(C)\n\t\n    copy_A = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{TA}, TA}(),\n                                    @Layout((32, 8)),\n                                    @Layout((1, 1)))\n    copy_B = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{TB}, TB}(),\n                                        @Layout((32, 8)),\n                                        @Layout((1, 1)))\n\n    mma_C = make_tiled_mma(UniversalFMA{TA,TB, TC}(), # MMA operation\n                           @Layout((32,8)))          # Atom layout\n\n    threads = Int(size(mma_C))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, copy_A,\n                                                      B, sB_layout, copy_B,\n                                                      C, mma_C)\nend\n\n\nfunction test()\n    A =  CUDA.randn(Float32, 2048, 256)\n    B =  CUDA.randn(Float32, 2048, 256)\n    C =  CUDA.randn(Float32, 2048, 2048)\n    matmul(A, B, C)\n    CUDA.synchronize()\n    @test C == A * B'\n    CUDA.unsafe_free!(A)\n    CUDA.unsafe_free!(B)\n    CUDA.unsafe_free!(C)\nend\n\ntest()","category":"page"},{"location":"manual/async/#Vectorized-copy","page":"Memcpy Async","title":"Vectorized copy","text":"","category":"section"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"We can change CPOP_ASYNC_CACHEALWAYS{TA}/CPOP_ASYNC_CACHEALWAYS{TB} to CPOP_ASYNC_CACHEALWAYS{Float64} to enable vectorized copies from global memory to shared memory. However, doing so will resul in a memory misaligned error. This is because we have padded sA and sB by one row. The element at [1,2] is not aligned to 8 bytes as required by the copy_async instruction, hence the error. We also need the following changes","category":"page"},{"location":"manual/async/","page":"Memcpy Async","title":"Memcpy Async","text":"sA_layout = make_layout((bM, bK), (_1, bM + _2))\nsB_layout = make_layout((bN, bK), (_1, bN + _2))","category":"page"},{"location":"manual/broadcast/#Broadcasting","page":"Broadcasting","title":"Broadcasting","text":"","category":"section"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"Broadcasting is only defined for MoYeArrays with static sizes. ","category":"page"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"In-place broadcasting preserves the original layout.","category":"page"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"Out-of-place broadcasting always returns an owning array of a compact layout with the same shape and the stride ordered the same.","category":"page"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"using MoYe\na = MoYeArray{Float64}(undef, @Layout((3,2), (2,1)))\nfill!(a, 1.0); \na .* 3\na .+ a","category":"page"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"b = MoYeArray{Float64}(undef, @Layout((3,), (2,))) |> zeros!; # Create a vector\na .- b ","category":"page"},{"location":"manual/broadcast/#On-GPU","page":"Broadcasting","title":"On GPU","text":"","category":"section"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"(In-place) broadcasting on device should just work:","category":"page"},{"location":"manual/broadcast/","page":"Broadcasting","title":"Broadcasting","text":"julia> function f()\n           a = MoYeArray{Float64}(undef, @Layout((3,2)))\n           fill!(a, one(eltype(a)))\n           a .= a .* 2\n           @cushow sum(a)\n           b = CUDA.exp.(a)\n           @cushow sum(b)\n           return nothing\n       end\nf (generic function with 1 method)\n\njulia> @cuda f()\nsum(a) = 12.000000\nsum(b) = 44.334337\nCUDA.HostKernel{typeof(f), Tuple{}}(f, CuFunction(Ptr{CUDA.CUfunc_st} @0x0000026e00ca1af0, CuModule(Ptr{CUDA.CUmod_st} @0x0000026e15cfc900, CuContext(0x0000026da1fff8b0, instance e5a1871b578f5adb))), CUDA.KernelState(Ptr{Nothing} @0x0000000204e00000))","category":"page"},{"location":"api/array/#MoYeArray","page":"MoYeArray","title":"MoYeArray","text":"","category":"section"},{"location":"api/array/#Index","page":"MoYeArray","title":"Index","text":"","category":"section"},{"location":"api/array/","page":"MoYeArray","title":"MoYeArray","text":"Pages = [\"array.md\"]","category":"page"},{"location":"api/array/#MoYe.ViewEngine","page":"MoYeArray","title":"MoYe.ViewEngine","text":"ViewEngine{T, P}\n\nA wrapper of a pointer. P is the type of the pointer.\n\n\n\n\n\n","category":"type"},{"location":"api/array/#MoYe.ArrayEngine","page":"MoYeArray","title":"MoYe.ArrayEngine","text":"ArrayEngine{T, L} <: DenseVector{T}\n\nA owning and mutable vector of type T with static length L.\n\nExamples\n\njulia> x = ArrayEngine{Float32}(undef, _3)\n3-element ArrayEngine{Float32, 3}:\n -9.8271385f-36\n  7.57f-43\n -9.8271385f-36\n\njulia> x[1] = 10f0\n10.0f0\n\njulia> x\n3-element ArrayEngine{Float32, 3}:\n 10.0\n  7.57f-43\n -9.8271385f-36\n\n\n\n\n\n","category":"type"},{"location":"api/array/#MoYe.MoYeArray","page":"MoYeArray","title":"MoYe.MoYeArray","text":"MoYeArray(engine::Engine, layout::Layout)\nMoYeArray{T}(::UndefInitializer, layout::StaticLayout)\nMoYeArray(ptr, layout::Layout)\n\nCreate a MoYeArray from an engine and a layout. See also ArrayEngine and ViewEngine.\n\nExamples\n\njulia> slayout = @Layout (5, 2);\n\njulia> array_engine = ArrayEngine{Float32}(undef, cosize(slayout)); # owning array\n\njulia> MoYeArray(array_engine, slayout)\n5×2 MoYeArray{Float32, 2, ArrayEngine{Float32, 10}, Layout{2, Tuple{Static.StaticInt{5}, Static.StaticInt{2}}, Tuple{Static.StaticInt{1}, Static.StaticInt{5}}}}:\n -3.24118f12   0.0\n  7.57f-43     0.0\n  0.0          0.0\n  0.0          0.0\n  7.89217f-40  0.0\n\njulia>  MoYeArray{Float32}(undef, slayout)\n5×2 MoYeArray{Float32, 2, ArrayEngine{Float32, 10}, Layout{2, Tuple{Static.StaticInt{5}, Static.StaticInt{2}}, Tuple{Static.StaticInt{1}, Static.StaticInt{5}}}}:\n  4.0f-45    7.57f-43\n  0.0        0.0\n -1.81623f7  0.0\n  7.57f-43   0.0\n -1.81623f7  0.0\n\njulia> A = ones(10);\n\njulia> MoYeArray(pointer(A), slayout) # non-owning array\n5×2 MoYeArray{Float64, 2, ViewEngine{Float64, Ptr{Float64}}, Layout{2, Tuple{Static.StaticInt{5}, Static.StaticInt{2}}, Tuple{Static.StaticInt{1}, Static.StaticInt{5}}}}:\n 1.0  1.0\n 1.0  1.0\n 1.0  1.0\n 1.0  1.0\n 1.0  1.0\n\njulia> function test_alloc()          # when powered by a ArrayEngine, MoYeArray is stack-allocated\n    slayout = @Layout (2, 3)          # and mutable\n    x = MoYeArray{Float32}(undef, slayout)\n    fill!(x, 1.0f0)\n    return sum(x)\nend\ntest_alloc (generic function with 2 methods)\n\njulia> @allocated(test_alloc())\n0\n\n\n\n\n\n","category":"type"},{"location":"api/array/#MoYe.recast","page":"MoYeArray","title":"MoYe.recast","text":"recast(::Type{NewType}, x::MoYeArray{OldType}) -> MoYeArray{NewType}\n\nRecast the element type of a MoYeArray. This is similar to Base.reinterpret, but dose all the computation at compile time, if possible.\n\nExamples\n\njulia> x = MoYeArray{Int32}(undef, @Layout((2,3)))\n2×3 MoYeArray{Int32, 2, ArrayEngine{Int32, 6}, Layout{2, Tuple{Static.StaticInt{2}, Static.StaticInt{3}}, Tuple{Static.StaticInt{1}, Static.StaticInt{2}}}}:\n -1948408944           0  2\n         514  -268435456  0\n\njulia> x2 = recast(Int16, x)\n4×3 MoYeArray{Int16, 2, ViewEngine{Int16, Ptr{Int16}}, Layout{2, Tuple{Static.StaticInt{4}, Static.StaticInt{3}}, Tuple{Static.StaticInt{1}, Static.StaticInt{4}}}}:\n -23664      0  2\n -29731      0  0\n    514      0  0\n      0  -4096  0\n\njulia> x3 = recast(Int64, x)\n1×3 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{1}, Static.StaticInt{3}}, Tuple{Static.StaticInt{1}, Static.StaticInt{1}}}}:\n 2209959748496  -1152921504606846976  2\n\n\n\n\n\n","category":"function"},{"location":"api/array/#MoYe.zeros!","page":"MoYeArray","title":"MoYe.zeros!","text":"zeros!(x::MoYeArray)\n\nFill x with zeros.\n\n\n\n\n\n","category":"function"},{"location":"manual/matmul/#MatMul","page":"MatMul","title":"MatMul","text":"","category":"section"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"(Image: matmuil)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"In this tutorial, we explore matrix multiplication using MoYe.jl , specifically computing the product C = A * B^top. Here, matrix A has dimensions (M K), matrix B has dimensions (K N), and the resulting matrix C will have dimensions (M N).","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"First, we divide the task among each block. We use a tile of size (bM, bN) to partition C, with each block responsible for computing one tile. The tile's index is determined by (blockIdx().x, blockIdx().y).","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"Computing a tile requires all values from A of shape (dM, K) and B of shape (dN, K). To reduce global memory access (since A, B, and C are stored in global memory), we further partition A and B along the K dimension, sequentially loading elements of sizes (dM, dK) and (dN, dK) into shared memory, then performing the matrix multiplication and accumulating the results into the tile of C.","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"The partition of the global memory corresponds to the following three lines of code:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"gC = @tile C (bM, bN) (blockIdx().x, blockIdx().y) # (bM, bN)\ngA = @tile A (bM, bK) (blockIdx().x, :)            # (bM, bK, K/bK)\ngB = @tile B (bN, bK) (blockIdx().y, :)            # (bN, bK, K/bK)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"For the specific partition syntax, please refer to @tile. Here, gA represents A in shared memory. Next, we use a for loop to index-slice the last dimension of gA and gB (denoted as k), loading them into shared memory. The code for this step is:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"sA = MoYeSharedArray(eltype(gA), sA_layout) # (bM, bK)\nsB = MoYeSharedArray(eltype(gB), sB_layout) # (bN, bK)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"MoYeSharedArray automatically allocates shared memory of size cosize(sA_layout) + cosize(sB_layout) and returns a MoYeArray. We will explain how to define the layouts for sA and sB later; for now, it's only necessary to know that they are predefined at compile time.","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"We then need to define how thread groups collectively copy from global to shared memory. There are many ways to organize threads, which will be discussed later, such as:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"tA = @Layout (32, 8)\ntB = @Layout (32, 8)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"This implies that there are 32x8 threads arranged in a column-major format. Next, we use them to partition the arrays:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"tAgA = @parallelize tA threadIdx().x       # (THR_M, THR_K, k)\ntBgB = @parallelize tB threadIdx().x       # (THR_M, THR_K)\n\ntAsA = @parallelize sA threadIdx().x       # (THR_N, THR_K, k)\ntBsB = @parallelize sB threadIdx().x       # (THR_N, THR_K)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"For the specific syntax, please refer to @parallelize. After the partition, copying is simply:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"copyto!(tAsA, view(tAgA, :, :, k))\ncopyto!(tBsB, view(tBgB, :, :, k))","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"After copying, we proceed to the actual matrix-multiply-accumulate (mma) computation. Similarly, we need to define a layout for the thread group for this purpose:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"tC = @Layout (16, 16)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"Then we use it to partition gC:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"tCgC = @parallelize gC tC threadIdx().x   # (THR_M, THR_N)\ntCrC = similar(tCgC)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"To reduce memory access to C, we also create an array tCrC stored in registers, which serves as the accumulator in the mma computation. After the computation, the contents are copied back into tCgC.","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"A and B are slightly different because computing an element in C requires an entire row from A and an entire column from B, which is reflected in the following code:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"tCsA = @parallelize sA tC threadIdx().x (1, :)    # (THR_M, bK)\ntCsB = @parallelize sB tC threadIdx().x (:, 1)    # (THR_N, bK)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"Congratulations, you have now completed all the partitions, and finally, we can compute the matrix multiplication, just as we would on a CPU:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"for k in axes(tCsA, 2)\n    for m in axes(tCsA, 1)\n        for n in axes(tCsB, 1)\n            @inbounds tCrC[m, n] += tCsA[m, k] * tCsB[n, k]\n        end\n    end\nend","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"You can also call [gemm!] to perform the same operation:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"gemm!(tCrC, tCsA, tCsB, tCrC)","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"The complete kernel code is as follows:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"function matmul_kernel(A, sA_layout, tA,\n                       B, sB_layout, tB,\n                       C, tC)\n    sA = MoYeSharedArray(eltype(A), sA_layout)           # (bM, bK)\n    sB = MoYeSharedArray(eltype(B), sB_layout)           # (bN, bK)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)              # (bM, bN)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)              # (bM, bK, K/bK)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)   # (bN, bK, K/bK)\n\n    # copy partition\n    tAgA = @parallelize gA tA threadIdx().x               # (THR_M, THR_K, k)\n    tBgB = @parallelize gB tB threadIdx().x               # (THR_M, THR_K)\n    tAsA = @parallelize sA tA threadIdx().x               # (THR_N, THR_K, k)\n    tBsB = @parallelize sB tB threadIdx().x               # (THR_N, THR_K)\n\n    # mma partition\n    tCsA = @parallelize sA tC threadIdx().x (1, :)        # (THR_M, bK)\n    tCsB = @parallelize sB tC threadIdx().x (:, 1)        # (THR_N, bK)\n    tCgC = @parallelize gC tC threadIdx().x               # (THR_M, THR_N)\n\n    # accumulator\n    tCrC = similar(tCgC)                                  # (THR_M, THR_N)\n    zeros!(tCrC)\n\n    for k in axes(tAgA, 3)\n        copyto!(tAsA, view(tAgA, :, :, k))\n        copyto!(tBsB, view(tBgB, :, :, k))\n        \n        cp_async_wait()\n        sync_threads()\n\n        @gc_preserve gemm!(tCrC, tCsA, tCsB, tCrC)\n        sync_threads()\n    end\n\n\n    copyto!(tCgC, tCrC)\n    return nothing\nend\n","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"We still missed a few points, such as:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"How to design sA_layout and sB_layout?","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"For shared memory, we no longer need to consider column-major or row-major but simply need to avoid bank conflicts. This can be simply achieved by padding one column.","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"sA_layout = make_layout((bM, bK), (_1, bM + _1))\nsB_layout = make_layout((bN, bK), (_1, bN + _1))","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"How to design tC?","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"The design of tC is quite flexible; it only needs to satisfy that the shape of tC evenly divides (bM, bN).","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"How to design tA and tB?","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"You generally want every 32 threads to access contiguous elements in A and B, so the specific design depends on the memory layout of A and B. This technique is known as memory coalescing.","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"The matmul function looks like this:","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"function matmul(A, B, C)\n    bM = _128\n    bN = _128\n    bK = _8\n    \n    sA_layout = make_layout((bM, bK), (_1, bM + _1))\n    sB_layout = make_layout((bN, bK), (_1, bN + _1))\n\n    tA = @Layout (32, 8)\n    tB = @Layout (32, 8)\n    tC = @Layout (16, 16)\n\n    threads = Int(size(tC))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, tA,\n                                                      B, sB_layout, tB,\n                                                      C, tC)\nend\n\nfunction test()\n    A =  CUDA.randn(Float32, 2048, 256)\n    B =  CUDA.randn(Float32, 2048, 256)\n    C =  CUDA.randn(Float32, 2048, 2048)\n    matmul(A, B, C)\n    CUDA.synchronize()\n    @test C == A * B'\n    CUDA.unsafe_free!(A)\n    CUDA.unsafe_free!(B)\n    CUDA.unsafe_free!(C)\nend\n\ntest()","category":"page"},{"location":"manual/matmul/","page":"MatMul","title":"MatMul","text":"This concludes the guide to implementing matrix multiplication with MoYe.jl, focusing on efficient memory management and","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Tiled Copy","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"We have already introduced how to copy data using @tile and @parallelize. This process might still appear somewhat cumbersome, and TiledCopy serves to simplify it.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Consider the following example where we employ six threads to transfer an array src of shape (4, 9) into another array dst with the identical shape. The relationship mapping logic coordinates to thread IDs can be visualized as:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"1 1 1 2 2 2 3 3 3\n1 1 1 2 2 2 3 3 3\n4 4 4 5 5 5 6 6 6\n4 4 4 5 5 5 6 6 6","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Here, each thread is assigned a data segment defined by the layout (2,3):(1,2). The group of threads operates within a layout of (2,3):(3,1), referred to as val_layout and thr_layout, respectively.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"To begin, we initialize these arrays:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"using MoYe\nsrc_buffer = collect(1:36) .* 0.1;\nsrc = MoYeArray(src_buffer, @Layout((4,9)))\ndst_buffer = zeros(36);\ndst = MoYeArray(dst_buffer, make_layout((_4,_9)));","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"We then proceed to set up a TiledCopy:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"thr_layout = @Layout (2, 3) (3, 1)\nval_layout = @Layout (2, 3) (1, 2)\ntiled_copy = make_tiled_copy(\n\tCopyAtom{UniversalCopy{Float64}, Float64}(),\n\tthr_layout, \n\tval_layout)","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"The second parameter Float64 in CopyAtom indicates that the copied data is of Float64 type. UniversalCopy{Float64} is used for vectorized copy operations, meaning that the data is recast to Float64, i.e., without vectorization. Here is a vectorized TiledCopy","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"tiled_copy_vec = make_tiled_copy(\n\tCopyAtom{UniversalCopy{UInt128}, Float64}(),\n\tthr_layout, \n\tval_layout)","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Note that vectorized copy must be comatiable with val_layout, i.e., val_layout needs to have enough and divisible number of elements to be vectorized.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"You can visualize this tiled_copy by using print_typst(tiled_copy). Visit typst, copy the printed string, and you will see the following image:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"(Image: matmuil)","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"The two tables respectively represent the thread distribution of src and dst, which are the same here. There are also some PTX instructions involved in reallocating each thread's data, for example:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"print_typst(make_tiled_copy(MoYe.CopyAtom{LDSM_U32x4_N, UInt16}(),\n                                          @Layout((16,2)), @Layout((2,4))));\n","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"(Image: matmuil)","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"As you can see, both thrlayout and vallayout are actually defined on dst.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"We will go back to ldmatrix when we talk about tensor cores.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Returning to our example, after making the tiled_copy, we can use it to partition data.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"thr_idx = 2;\nthr_copy = get_slice(tiled_copy, thr_idx);\ndst_t = partition_D(thr_copy, dst);\ndst_t.layout\nsrc_t = partition_S(thr_copy, src);\nsrc_t.layout\ncopyto!(tiled_copy, dst_t, src_t);\ndst","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"You can see that the second thread has completed the copy. The shape of dst_t is (CPY, CPY_M, CPY_K) representing the the num of values handle by a thread in a single tile, and the demensions tiled in dst's shape. Notably, the left most mode of CPY stands for the number of vectorized values. In this case it is 1, but try changing to UniversalCopy{UInt128} and see how the result changes.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"The NVIDIA Ampere architecture supports cuda::memcpy_async for asynchronously copying data between GPU global memory and shared memory without needing threads to orchestrate the data movement. In previous architectures, copying from global memory to shared memory usually involved registers for intermediation, corresponding to this syntax:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"thr_idx = 3;\nthr_copy = get_slice(tiled_copy, thr_idx);\ndst_t = partition_D(thr_copy, dst);\nsrc_t = partition_S(thr_copy, src);\n\ndst_r = make_fragment_like(dst_t);\ncopyto!(tiled_copy, dst_r, src_t);\ncopyto!(tiled_copy, dst_t, dst_r);\ndst","category":"page"},{"location":"manual/tiled_matmul/#TiledMMA","page":"TiledCopy & TiledMMA","title":"TiledMMA","text":"","category":"section"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"In this section, we'll show you how to use TiledMMA to replace an mma partition. First, invoke the function maketiledmma as follows:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"mma_C = make_tiled_mma(UniversalFMA{TA,TB, TC}(), # MMA operation\n                       @Layout((16,16)))          # Atom layout\n","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"You can experiment with replacing UniversalFMA with another MMAOp and use print_typst to view the results. Here are the predefined MMAOps:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"MoYe.mma_ops_list","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"thr_mma = get_slice(mma_C, threadIdx().x);\ntCsA = partition_A(sA);\ntCsB = partition_B(sB);\ntCgC = partition_C(gC);\n\ntCrC = make_fragment_like(tCgC)","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"These instructions operate on tensor cores, a topic we haven't covered yet (but will soon!).","category":"page"},{"location":"manual/tiled_matmul/#MatMul","page":"TiledCopy & TiledMMA","title":"MatMul","text":"","category":"section"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Now, we use TiledCopy and TiledMMA to upgrade the previous matmul_kernel.","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"function matmul_kernel(A, sA_layout, copy_A,\n                       B, sB_layout, copy_B,\n                       C, mma_C)\n    sA = MoYeSharedArray(eltype(A), sA_layout)\n    sB = MoYeSharedArray(eltype(B), sB_layout)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n    \n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # copy partition\n    thr_copy_a = get_slice(copy_A, threadIdx().x)      \n    tAgA = partition_S(thr_copy_a, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(thr_copy_a, sA)                 # (CPY, CPY_M, CPY_K)\n    tArA = make_fragment_like(tAsA)                    # (CPY, CPY_M, CPY_K)\n\n    thr_copy_b = get_slice(copy_B, threadIdx().x)\n    tBgB = partition_S(thr_copy_b, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(thr_copy_b, sB)                 # (CPY, CPY_N, CPY_K)\n    tBrB = make_fragment_like(tBsB)                    # (CPY, CPY_N, CPY_K)\n\n    # mma partition\n    thr_mma = get_slice(mma_C, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                    # (MMA, MMA_M, MMA_K)\n    tCsB = partition_B(thr_mma, sB)                    # (MMA, MMA_M, MMA_K)\n    tCgC = partition_C(thr_mma, gC)                    # (MMA, MMA_M, MMA_N)\n\n    # overlap copy and compute\n    copyto!(copy_A, tArA, view(tAgA, :, :, :, _1))\n    copyto!(copy_B, tBrB, view(tBgB, :, :, :, _1))\n\n    # accumulator\n    tCrC = make_fragment_C(thr_mma, tCgC)\n    zeros!(tCrC)\n\n    k_max = size(tAgA, 4)\n    for k in 1:k_max\n        sync_threads()\n        copyto!(tAsA, tArA)\n        copyto!(tBsB, tBrB)\n        sync_threads()\n\n\t    # load the next tile\n\t    k_next = k < k_max ? k+1 : k\n\t    copyto!(copy_A, tArA, view(tAgA, :, :, :, k_next))\n\t    copyto!(copy_B, tBrB, view(tBgB, :, :, :, k_next))\n\n        @gc_preserve gemm!(mma_C, tCrC, tCsA, tCsB, tCrC)\n    end\n\n    copyto!(tCgC, tCrC)\n    return nothing\nend\n\n\nfunction matmul(A, B, C)\n    bM = _128\n    bN = _128\n    bK = _8\n    \n    sA_layout = make_layout((bM, bK), (_1, bM + _1))\n    sB_layout = make_layout((bN, bK), (_1, bN + _1))\n\n    TA = eltype(A)\n    TB = eltype(B)\n    TC = eltype(C)\n\t\n    copy_A = make_tiled_copy(CopyAtom{UniversalCopy{TA}, TA}(),\n                             @Layout((32, 8)),\n                             @Layout((1, 1)))\n    copy_B = make_tiled_copy(CopyAtom{UniversalCopy{TB}, TB}(),\n                             @Layout((32, 8)),\n                             @Layout((1, 1)))\n\n    mma_C = make_tiled_mma(UniversalFMA{TA,TB, TC}(), # MMA operation\n                           @Layout((32,8)))          # Atom layout\n\n    threads = Int(size(mma_C))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, copy_A,\n                                                      B, sB_layout, copy_B,\n                                                      C, mma_C)\nend\n\nfunction test()\n    A =  CUDA.randn(Float32, 2048, 256)\n    B =  CUDA.randn(Float32, 2048, 256)\n    C =  CUDA.randn(Float32, 2048, 2048)\n    matmul(A, B, C)\n    CUDA.synchronize()\n    @test C == A * B'\n    CUDA.unsafe_free!(A)\n    CUDA.unsafe_free!(B)\n    CUDA.unsafe_free!(C)\nend\n\ntest()","category":"page"},{"location":"manual/tiled_matmul/#Vectorized-copy","page":"TiledCopy & TiledMMA","title":"Vectorized copy","text":"","category":"section"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"As previously mentioned, you can change to UniversalCopy{Float64} or UniversalCopy{UInt128} to enabled vectoried copy. But we also need to keep in mind the copies are coalesced. For example, the following one is not coalesced","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"copy_A = make_tiled_copy(CopyAtom{UniversalCopy{Float64}, TA}(),\n                             @Layout((32, 8)),\n                             @Layout((4, 1)))","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"since thread 1 is loading from [1], [2] and thead 2 is loading from [5], [6].","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"Theses are coalesced:","category":"page"},{"location":"manual/tiled_matmul/","page":"TiledCopy & TiledMMA","title":"TiledCopy & TiledMMA","text":"copy_A = make_tiled_copy(CopyAtom{UniversalCopy{Float64}, TA}(),\n                             @Layout((32, 8)),\n                             @Layout((2, 1)))\ncopy_A = make_tiled_copy(CopyAtom{UniversalCopy{UInt128}, TA}(),\n                             @Layout((32, 8)),\n                             @Layout((4, 1)))          ","category":"page"},{"location":"manual/tensor_core/#Tensor-Cores","page":"Tensor Cores","title":"Tensor Cores","text":"","category":"section"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"Tensor cores are specialized hardware accelerators designed to optimize matrix operations, which are crucial for deep learning and artificial intelligence algorithms.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"Enabling tensor cores can be as straightforward as modifying a single line of code in the existing matmul_kernel function:","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"mma = make_tiled_mma(MMAOP_8x8x4_F32F16F16F32_NT(), \n                     atom_layout, \n                     tiler)","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"note: Note\nThe NT in MMAOP8x8x4F32F16F16F32_NT indicates that A is in M-major order and B is in N-major order.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"Let's explore a minimal example","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"mma = make_tiled_mma(MMAOP_16x8x8_F32TF32TF32F32_TN())\nprint_typst(mma)","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"(Image: )","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"At first glance, the diagram may seem complex, but the concept is straightforward: the threads collective load data from matrices A and B according to the specified layout. During the matrix multiply-accumulate (MMA) computation, data is internally shared among threads—a process that is not transparent to the user. Once the computation is complete, each thread stores the results as dictated by the layout of matrix C shown in the illustration.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"function matmul_kernel(A, sA_layout, copy_A,\n                       B, sB_layout, copy_B,\n                       C, mma)\n    sA = MoYeSharedArray(eltype(A), sA_layout)\n    sB = MoYeSharedArray(eltype(B), sB_layout)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # copy partition\n    thr_copy_a = get_slice(copy_A, threadIdx().x)      \n    tAgA = partition_S(thr_copy_a, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(thr_copy_a, sA)                 # (CPY, CPY_M, CPY_K)\n\n    thr_copy_b = get_slice(copy_B, threadIdx().x)\n    tBgB = partition_S(thr_copy_b, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(thr_copy_b, sB)                 # (CPY, CPY_N, CPY_K)\n\n    # mma partition\n    thr_mma = get_slice(mma, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                    # (MMA, MMA_M, MMA_K)\n    tCsB = partition_B(thr_mma, sB)                    # (MMA, MMA_M, MMA_K)\n    tCgC = partition_C(thr_mma, gC)                    # (MMA, MMA_M, MMA_N)\n\n    tCrA = make_fragment_A(thr_mma, tCsA)              # (MMA, MMA_M, MMA_K)\n    tCrB = make_fragment_B(thr_mma, tCsB) \n    tCrC = make_fragment_C(thr_mma, tCgC)\n    zeros!(tCrC)\n\n    # copy from global to shared\n    copyto!(copy_A, tAsA, view(tAgA, :, :, :, _1))\n    copyto!(copy_B, tBsB, view(tBgB, :, :, :, _1))\n    \n    cp_async_wait()\n\n    # copy from shared to registers\n    copyto!(tCrA, tCsA)\n    copyto!(tCrB, tCsB)\n\n    @gc_preserve gemm!(mma, tCrC, tCrA, tCrB, tCrC)\n\n    copyto!(tCgC, tCrC) \n    @inbounds tCrC[1]  # compiler bug, have to load after copyto!\n\n    return nothing\nend\n\nfunction matmul(A, B, C)\n    bM = _16\n    bN = _8\n    bK = _8\n    \n    sA_layout = make_layout((bM, bK), (_1, bM))\n    sB_layout = make_layout((bN, bK), (bK, _1))\n\n    TA = eltype(A)\n    TB = eltype(B)\n    TC = eltype(C)\n\t\n    copy_A = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{UInt128}, TA}(),\n                             @Layout((4, 8)),\n                             @Layout((4, 1)))\n    copy_B = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{UInt64}, TB}(),\n                             @Layout((8, 4), (4, 1)),\n                             @Layout((1, 2)))\n\n    mma = make_tiled_mma(MMAOP_16x8x8_F32TF32TF32F32_TN()) \n\n    threads = Int(size(mma))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, copy_A, \n                                                      B, sB_layout, copy_B, \n                                                      C, mma)\nend\n\nfunction test()\n    A =  CuArray(reshape(collect(1:16*8) .* 1f0, (16,8))) \n    B =  CuArray(reshape(collect(1:8*8) .* 1f0, (8,8)))\n    C =  CuArray(ones(Float32, (16,8)))\n    matmul(A, B', C)\n    CUDA.synchronize()\n    @test C == A * B\n    CUDA.unsafe_free!(A)\n    CUDA.unsafe_free!(B)\n    CUDA.unsafe_free!(C)\nend","category":"page"},{"location":"manual/tensor_core/#LDMatrix","page":"Tensor Cores","title":"LDMatrix","text":"","category":"section"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"The ldmatrix instruction at the warp level facilitates the loading of data from shared memory into registers and suffles them to align with a tensor core MMA operation.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"Given a tensor core MMA operation, the shuffling can be \"inverted\" to obtain a TiledCopy count for the shuffling.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"mma = make_tiled_mma(MMAOP_16x8x8_F32TF32TF32F32_TN())\nsmem_copy_A = make_tiled_copy_A(CopyAtom{LDSM_U32x4_N, Float32}(), mma)\nprint_typst(smem_copy_A)","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"(Image: )","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"The resulting layout on the right hand side matches the layout of A in the mma.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"note: Note\nThe TN in MMAOP_16x8x8_F32TF32TF32F32_TN means that both A and B are in K-major order. The N in LDSM_U32x4_N means the source array is K-major order.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"note: Note\nThe ldmatrix requires four consecutive threads to load 16 consecutive bytes, demanding that the layout of A in shared memory meet this specification.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"For B:","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"smem_copy_B = make_tiled_copy_B(CopyAtom{LDSM_U32x2_N, Float32}(), mma)\nprint_typst(smem_copy_B)","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"(Image: )","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"!!! Note     The visualization of B in mma is draw as (K, N) but (N, K) in smem_copy_B.","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"We then use smem_copy_A and smem_copy_B to re-tile the shared memory and registers","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"smem_thr_copy_A = get_slice(smem_copy_A, threadIdx().x)\nsmem_thr_copy_B = get_slice(smem_copy_B, threadIdx().x)\ntCsA_retiled = partition_S(smem_thr_copy_A, sA)\ntCsB_retiled = partition_S(smem_thr_copy_B, sB)\ntCrA_retiled = retile_D(smem_thr_copy_A, tCrA)\ntCrB_retiled = retile_D(smem_thr_copy_B, tCrB)","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"Complete code:","category":"page"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"function matmul_kernel(A, sA_layout, gmem_copy_A, smem_copy_A,\n                       B, sB_layout, gmem_copy_B, smem_copy_B,\n                       C, mma)\n    sA = MoYeSharedArray(eltype(A), sA_layout)\n    sB = MoYeSharedArray(eltype(B), sB_layout)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # gmem copy partition\n    gmem_thr_copy_a = get_slice(gmem_copy_A, threadIdx().x)      \n    tAgA = partition_S(gmem_thr_copy_a, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(gmem_thr_copy_a, sA)                 # (CPY, CPY_M, CPY_K)\n\n    gmem_thr_copy_b = get_slice(gmem_copy_B, threadIdx().x)\n    tBgB = partition_S(gmem_thr_copy_b, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(gmem_thr_copy_b, sB)                 # (CPY, CPY_N, CPY_K)\n\n    # copy from global to shared\n    copyto!(gmem_copy_A, tAsA, view(tAgA, :, :, :, _1))\n    copyto!(gmem_copy_B, tBsB, view(tBgB, :, :, :, _1))\n\n    # mma partition\n    thr_mma = get_slice(mma, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                    # (MMA, MMA_M, MMA_K)\n    tCsB = partition_B(thr_mma, sB)                    # (MMA, MMA_M, MMA_K)\n    tCgC = partition_C(thr_mma, gC)                    # (MMA, MMA_M, MMA_N)\n\n    tCrA = make_fragment_A(thr_mma, tCsA)              # (MMA, MMA_M, MMA_K)\n    tCrB = make_fragment_B(thr_mma, tCsB)              # (MMA, MMA_N, MMA_K)\n    tCrC = make_fragment_C(thr_mma, tCgC)              # (MMA, MMA_M, MMA_N)\n    zeros!(tCrC)\n\n    # retile \n    smem_thr_copy_A = get_slice(smem_copy_A, threadIdx().x)\n    smem_thr_copy_B = get_slice(smem_copy_B, threadIdx().x)\n    tCsA_retiled = partition_S(smem_thr_copy_A, sA)\n    tCsB_retiled = partition_S(smem_thr_copy_B, sB)\n    tCrA_retiled = retile_D(smem_thr_copy_A, tCrA)\n    tCrB_retiled = retile_D(smem_thr_copy_B, tCrB)\n    \n    cp_async_wait()\n\n    # copy from shared to registers\n    copyto!(smem_copy_A, tCrA_retiled, tCsA_retiled)\n    copyto!(smem_copy_B, tCrB_retiled, tCsB_retiled)\n\n    @gc_preserve gemm!(mma, tCrC, tCrA, tCrB, tCrC)\n\n    copyto!(tCgC, tCrC) \n    @inbounds tCrC[1]  # compiler bug, have to load after copyto!\n\n    return nothing\nend\n\n\nfunction matmul(A, B, C)\n    bM = _16\n    bN = _8\n    bK = _8\n    \n    sA_layout = make_layout((bM, bK), (_1, bM))\n    sB_layout = make_layout((bN, bK), (bK, _1))\n\n    TA = eltype(A)\n    TB = eltype(B)\n    TC = eltype(C)\n\t\n    gmem_copy_A = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{UInt128}, TA}(),\n                                  @Layout((4, 8)),\n                                  @Layout((4, 1)))\n    gmem_copy_B = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{UInt64}, TB}(),\n                                  @Layout((8, 4), (4, 1)),\n                                  @Layout((1, 2)))\n\n    mma = make_tiled_mma(MMAOP_16x8x8_F32TF32TF32F32_TN()) \n\n    # Note: A is M-major so we can only use `UniversalCopy`\n    smem_copy_A =  make_tiled_copy_A(CopyAtom{UniversalCopy{TA}, TA}(), mma)\n    smem_copy_B =  make_tiled_copy_B(CopyAtom{LDSM_U32x2_N, TB}(), mma)\n\n    threads = Int(size(mma))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, gmem_copy_A, smem_copy_A,\n                                                      B, sB_layout, gmem_copy_B, smem_copy_B,\n                                                      C, mma)\nend\n","category":"page"},{"location":"manual/tensor_core/#Double-buffering","page":"Tensor Cores","title":"Double buffering","text":"","category":"section"},{"location":"manual/tensor_core/","page":"Tensor Cores","title":"Tensor Cores","text":"@views function matmul_kernel(A, sA_layout, gmem_copy_A, smem_copy_A,\n                              B, sB_layout, gmem_copy_B, smem_copy_B,\n                              C, mma)\n    sA = MoYeSharedArray(eltype(A), sA_layout)        # (bM, bK, 2)\n    sB = MoYeSharedArray(eltype(B), sB_layout)        # (bN, bK, 2)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # gmem copy partition\n    gmem_thr_copy_A = get_slice(gmem_copy_A, threadIdx().x)      \n    tAgA = partition_S(gmem_thr_copy_A, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(gmem_thr_copy_A, sA)                 # (CPY, CPY_M, CPY_K, 2)\n\n    gmem_thr_copy_B = get_slice(gmem_copy_B, threadIdx().x)\n    tBgB = partition_S(gmem_thr_copy_B, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(gmem_thr_copy_B, sB)                 # (CPY, CPY_N, CPY_K, 2)\n\n    # Copy gmem to smem for k_tile=1\n    copyto!(gmem_copy_A, tAsA[:, :, :, _1], tAgA[:, :, :, _1])\n    copyto!(gmem_copy_B, tBsB[:, :, :, _1], tBgB[:, :, :, _1])\n\n    # mma partition\n    thr_mma = get_slice(mma, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                         # (MMA, MMA_M, MMA_K, 2)\n    tCsB = partition_B(thr_mma, sB)                         # (MMA, MMA_M, MMA_K, 2)\n    tCgC = partition_C(thr_mma, gC)                         # (MMA, MMA_M, MMA_N)\n\n    tCrA = make_fragment_A(thr_mma, tCsA[:, :, :, _1])      # (MMA, MMA_M, MMA_K)\n    tCrB = make_fragment_B(thr_mma, tCsB[:, :, :, _1])      # (MMA, MMA_N, MMA_K)\n    tCrC = make_fragment_C(thr_mma, tCgC)                   # (MMA, MMA_M, MMA_N)\n    zeros!(tCrC)\n\n    # retile \n    smem_thr_copy_A = get_slice(smem_copy_A, threadIdx().x)   \n    smem_thr_copy_B = get_slice(smem_copy_B, threadIdx().x)   \n    tCsA_retiled = partition_S(smem_thr_copy_A, sA)         # (MMA, MMA_M, MMA_K, 2)   \n    tCsB_retiled = partition_S(smem_thr_copy_B, sB)         # (MMA, MMA_N, MMA_K, 2)      \n    tCrA_retiled = retile_D(smem_thr_copy_A, tCrA)          # (MMA, MMA_M, MMA_K)    \n    tCrB_retiled = retile_D(smem_thr_copy_B, tCrB)          # (MMA, MMA_N, MMA_K)    \n\n    cp_async_wait()\n    sync_threads()\n\n    # Copy smem to rmem for k_block=1\n    smem_read = 1\n    smem_write = 2\n    tCsA_p = view(tCsA_retiled, :, :, :, smem_read)\n    tCsB_p = view(tCsB_retiled, :, :, :, smem_read)\n    copyto!(smem_copy_A, tCrA_retiled[:, :, _1], tCsA_p[:, :, _1])\n    copyto!(smem_copy_B, tCrB_retiled[:, :, _1], tCsB_p[:, :, _1])\n\n    k_tile_max = size(tAgA, 4)\n    k_block_max = static_size(tCrA, 3)\n    for k_tile in 1:k_tile_max\n        @loopinfo unroll for k_block in _1:k_block_max\n            k_block_next = k_block + 1 \n            if k_block == k_block_max\n                cp_async_wait()\n                sync_threads()\n                tCsA_p = view(tCsA_retiled, :, :, :, smem_read)\n                tCsB_p = view(tCsB_retiled, :, :, :, smem_read)\n                k_block_next = 1\n            end\n            \n            copyto!(smem_copy_A, tCrA_retiled[:, :, k_block_next], tCsA_p[:, :, k_block_next])\n            copyto!(smem_copy_B, tCrB_retiled[:, :, k_block_next], tCsB_p[:, :, k_block_next])   \n\n            if k_block == _1 && k_tile<k_tile_max\n                copyto!(gmem_copy_A, tAsA[:, :, :, smem_write], tAgA[:, :, :, k_tile+1])\n                copyto!(gmem_copy_B, tBsB[:, :, :, smem_write], tBgB[:, :, :, k_tile+1])\n                smem_read, smem_write = smem_write, smem_read\n            end\n            \n            @gc_preserve gemm!(mma, tCrC, tCrA[:, :, k_block], tCrB[:, :, k_block], tCrC)\n        end\n    end\n\n    copyto!(tCgC, tCrC)\n    sync_threads()\n    return nothing\nend\n\n\nfunction matmul(A, B, C)\n    bM = _128\n    bN = _128\n    bK = _16\n\n    TA = eltype(A)\n    TB = eltype(B)\n    TC = eltype(C)\n\t\n    mma = make_tiled_mma(MMAOP_16x8x8_F32TF32TF32F32_TN(), \n                         @Layout((2,2,1), (2,1,1)),\n                         (_32,_32,_8))\n\n    gmem_copy_A = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{UInt128}, TA}(),\n                                  @Layout((16, 8)),\n                                  @Layout((4, 1)))\n    gmem_copy_B = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{UInt128}, TB}(),\n                                  @Layout((32, 4), (4, 1)),\n                                  @Layout((1, 4)))\n\n    # A is M-major so we cannot use LDSM_U32x4_N \n    smem_copy_A = make_tiled_copy_A(CopyAtom{UniversalCopy{TA}, TA}(), mma)\n    smem_copy_B = make_tiled_copy_B(CopyAtom{LDSM_U32x4_N, TB}(), mma)\n\n    sA_layout = @Layout (128, 16, 2) (1, 128, 2048)\n    sB_layout = @Layout (128, 16, 2) (16, 1,  2048)\n \n    threads = Int(size(mma))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, gmem_copy_A, smem_copy_A,\n                                                      B, sB_layout, gmem_copy_B, smem_copy_B,\n                                                      C, mma)\nend\n\n\nfunction test()\n    A = CUDA.randn(Float32, 2048, 256)   # M-major\n    B = CUDA.randn(Float32, 256, 2048)   # K-major\n    C =  CUDA.randn(Float32, 2048, 2048)\n    matmul(A, B', C)\n    CUDA.synchronize()\n    @test C == A * B\n    CUDA.unsafe_free!(A)\n    CUDA.unsafe_free!(B)\n    CUDA.unsafe_free!(C)\nend\n\ntest()","category":"page"},{"location":"api/layout/#Layout","page":"Layout","title":"Layout","text":"","category":"section"},{"location":"api/layout/#Index","page":"Layout","title":"Index","text":"","category":"section"},{"location":"api/layout/","page":"Layout","title":"Layout","text":"Pages = [\"layout.md\"]","category":"page"},{"location":"api/layout/#Constructors","page":"Layout","title":"Constructors","text":"","category":"section"},{"location":"api/layout/#MoYe.Layout","page":"Layout","title":"MoYe.Layout","text":"Layout{N, Shape, Stride}\n\nA Layout is a pair of Shape and Stride tuples.  The Shape tuple contains the number of elements in each dimension, and the Stride tuple contains the number of elements to skip to get to the next element in each dimension.\n\nFields\n\nshape.\nstride.\n\nIndexing\n\nA Layout can be indexed with three types of indices:\n\nInt: a linear index in a column-major order.\nIntTuple: a hierarchical index. It has the exact hierarchical structure as defined by the Shape.\nIntTuple: a congruent index. A tuple of N mixes hierarchical and linear indices along each dimension.\n\nExamples\n\njulia> layout = Layout((4, (2, 2)), (2, (1, 8)));\n\njulia> print_layout(ans)\n(4, (2, 2)):(2, (1, 8))\n       1    2    3    4\n    +----+----+----+----+\n 1  |  1 |  2 |  9 | 10 |\n    +----+----+----+----+\n 2  |  3 |  4 | 11 | 12 |\n    +----+----+----+----+\n 3  |  5 |  6 | 13 | 14 |\n    +----+----+----+----+\n 4  |  7 |  8 | 15 | 16 |\n    +----+----+----+----+\n\njulia> layout(6) # linear index\n4\n\njulia> layout((2,2)) # hierarchical index\n4\n\njulia> layout((2,(2,1))) # congruent index\n4\n\n\n\n\n\n","category":"type"},{"location":"api/layout/#MoYe.@Layout","page":"Layout","title":"MoYe.@Layout","text":"@Layout(shape, stride=nothing)\n\nConstruct a static layout with the given shape and stride.\n\nArguments\n\nshape: a tuple of integers or a single integer\nstride: a tuple of integers, a single integer, GenColMajor or GenRowMajor\n\n\n\n\n\n","category":"macro"},{"location":"api/layout/#MoYe.make_layout","page":"Layout","title":"MoYe.make_layout","text":"make_layout(shape::IntTuple, stride::IntTuple)\nmake_layout(shape::IntTuple, major=GenColMajor)\n\nConstruct a layout with the given shape and stride. If the stride is not given, it is set to col-major compact stride. See alse GenColMajor and GenRowMajor.\n\n\n\n\n\nmake_layout(::Layouts...)\n\nConcatenate layouts into a single layout.\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#Fundamentals","page":"Layout","title":"Fundamentals","text":"","category":"section"},{"location":"api/layout/#Base.size-Tuple{Layout}","page":"Layout","title":"Base.size","text":"size(::Layout)\nsize(::Layout, i::Union{Int, StaticInt})\n\nGet the cardinality of the domain of the layout. See also cosize.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#MoYe.rank-Tuple{Layout}","page":"Layout","title":"MoYe.rank","text":"rank(::Layout)\nrank(::Layout, i::Union{Int, StaticInt})\n\nGet the rank, i.e., the dimensionality, of the layout.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#MoYe.depth-Tuple{Layout}","page":"Layout","title":"MoYe.depth","text":"depth(::Layout)\ndepth(::Layout, i::Union{Int, StaticInt})\n\nGet the depth of the hierarchy of the layout. For example, the depth of (1,2) is 1, and the depth of ((1,2),3) is 2.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#MoYe.cosize-Tuple{Layout}","page":"Layout","title":"MoYe.cosize","text":"cosize(::Layout)\ncosize(::Layout, i::Union{Int, StaticInt})\n\nGet the cardinality of the codomain of the layout. See also size.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#Base.getindex-Tuple{Layout, Vararg{Union{Int32, Int64, Static.StaticInt}}}","page":"Layout","title":"Base.getindex","text":"getindex(layout::Layout, Is...)\n\nGet the sub-layout of layout with the given indices.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#Compact-Layout","page":"Layout","title":"Compact Layout","text":"","category":"section"},{"location":"api/layout/#MoYe.GenColMajor","page":"Layout","title":"MoYe.GenColMajor","text":"GenColMajor\n\nmake_layout uses this to create a col-major compact layout.\n\njulia> make_layout(((1, (2, 4)), 1), MoYe.GenColMajor)\n((1, (2, 4)), 1):((_1, (1, 2)), 8)\n\n\n\n\n\n","category":"type"},{"location":"api/layout/#MoYe.GenRowMajor","page":"Layout","title":"MoYe.GenRowMajor","text":"GenRowMajo\n\nmake_layout uses this to create a row-major compact layout.\n\njulia> make_layout(((1, (2, 4)), 1), MoYe.GenRowMajor)\n((1, (2, 4)), 1):((8, (4, 1)), _1)\n\n\n\n\n\n","category":"type"},{"location":"api/layout/#Algebra","page":"Layout","title":"Algebra","text":"","category":"section"},{"location":"api/layout/#Concatenation","page":"Layout","title":"Concatenation","text":"","category":"section"},{"location":"api/layout/#Base.cat-Tuple{Vararg{Layout}}","page":"Layout","title":"Base.cat","text":"cat(::Layouts...)\n\nConcatenate layouts into a single layout.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#MoYe.make_layout-Tuple{Vararg{Layout}}","page":"Layout","title":"MoYe.make_layout","text":"make_layout(::Layouts...)\n\nConcatenate layouts into a single layout.\n\n\n\n\n\n","category":"method"},{"location":"api/layout/#Composition","page":"Layout","title":"Composition","text":"","category":"section"},{"location":"api/layout/#MoYe.composition","page":"Layout","title":"MoYe.composition","text":"composition(l1::Layout, l2::Layout)\n\nCompose two layouts as composing two functions. You can use ∘ operator as well.\n\nExamples\n\njulia> make_layout(20, 2) ∘ make_layout((4, 5), (1, 4))\n(4, 5):(2, 8)\n\n\njulia> make_layout(20, 2) ∘ make_layout((4, 5), (5, 1))\n(4, 5):(10, 2)\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#Complement","page":"Layout","title":"Complement","text":"","category":"section"},{"location":"api/layout/#MoYe.complement","page":"Layout","title":"MoYe.complement","text":"complement(l::Layout, cosize::IntType)\n\nA complement layout of A is a layout B such that (A, B) is a compact layout of size cosize.\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#Inverse","page":"Layout","title":"Inverse","text":"","category":"section"},{"location":"api/layout/#MoYe.left_inverse","page":"Layout","title":"MoYe.left_inverse","text":"left_inverse(layout::Layout)\n\nReturn the left inverse of layout, i.e. a layout layout′ such that (layout′ ∘ layout)(i) == (i). The domain of layout′ is chosen to be the maximum continues squence of the domain of layout.\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#MoYe.right_inverse","page":"Layout","title":"MoYe.right_inverse","text":"right_inverse(layout::Layout)\n\nReturn the right inverse of layout, i.e. a layout layout′ such that (layout ∘ layout′)(i) == (i). The domain of layout′ is chosen to be the maximum continues squence of the codomain of layout.\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#Product","page":"Layout","title":"Product","text":"","category":"section"},{"location":"api/layout/#MoYe.logical_product","page":"Layout","title":"MoYe.logical_product","text":"logical_product(A::Layout, B::Layout)\n\nCompute the logical product of two layouts. Indexing through the first mode of the resulting layout corresponds to indexing through A and indexing through the second mode corresponds to indexing through B.\n\njulia> tile = @Layout((2, 2), (1, 2));\n\njulia> print_layout(tile)\n(_2, _2):(_1, _2)\n      1   2\n    +---+---+\n 1  | 1 | 3 |\n    +---+---+\n 2  | 2 | 4 |\n    +---+---+\n\njulia> matrix_of_tiles = @Layout((3, 4), (4, 1));\n\njulia> print_layout(matrix_of_tiles)\n(_3, _4):(_4, _1)\n       1    2    3    4\n    +----+----+----+----+\n 1  |  1 |  2 |  3 |  4 |\n    +----+----+----+----+\n 2  |  5 |  6 |  7 |  8 |\n    +----+----+----+----+\n 3  |  9 | 10 | 11 | 12 |\n    +----+----+----+----+\n\njulia> print_layout(logical_product(tile, matrix_of_tiles))\n((_2, _2), (_3, _4)):((_1, _2), (_16, _4))\n       1    2    3    4    5    6    7    8    9   10   11   12\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 1  |  1 | 17 | 33 |  5 | 21 | 37 |  9 | 25 | 41 | 13 | 29 | 45 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 2  |  2 | 18 | 34 |  6 | 22 | 38 | 10 | 26 | 42 | 14 | 30 | 46 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 3  |  3 | 19 | 35 |  7 | 23 | 39 | 11 | 27 | 43 | 15 | 31 | 47 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 4  |  4 | 20 | 36 |  8 | 24 | 40 | 12 | 28 | 44 | 16 | 32 | 48 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#MoYe.blocked_product","page":"Layout","title":"MoYe.blocked_product","text":"blocked_product(tile::Layout, matrix_of_tiles::Layout, coalesce_result::Bool=false)\n\nCompute the blocked product of two layouts. Indexing through the first mode of the resulting layout corresponds to indexing through the cartesian product of the first mode of tile and the first mode of matrix_of_tiles. Indexing through the second mode is similar. If coalesce_result is true, then the result is coalesced.\n\njulia> tile = @Layout (2, 2);\n\njulia> matrix_of_tiles = @Layout (3, 4) (4, 1);\n\njulia> print_layout(blocked_product(tile, matrix_of_tiles))\n((_2, _3), (_2, _4)):((_1, _16), (_2, _4))\n       1    2    3    4    5    6    7    8\n    +----+----+----+----+----+----+----+----+\n 1  |  1 |  3 |  5 |  7 |  9 | 11 | 13 | 15 |\n    +----+----+----+----+----+----+----+----+\n 2  |  2 |  4 |  6 |  8 | 10 | 12 | 14 | 16 |\n    +----+----+----+----+----+----+----+----+\n 3  | 17 | 19 | 21 | 23 | 25 | 27 | 29 | 31 |\n    +----+----+----+----+----+----+----+----+\n 4  | 18 | 20 | 22 | 24 | 26 | 28 | 30 | 32 |\n    +----+----+----+----+----+----+----+----+\n 5  | 33 | 35 | 37 | 39 | 41 | 43 | 45 | 47 |\n    +----+----+----+----+----+----+----+----+\n 6  | 34 | 36 | 38 | 40 | 42 | 44 | 46 | 48 |\n    +----+----+----+----+----+----+----+----+\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#MoYe.raked_product","page":"Layout","title":"MoYe.raked_product","text":"raked_product(tile::Layout, matrix_of_tiles::Layout, coalesce_result::Bool=false)\n\nThe tile is shattered or interleaved with the matrix of tiles.\n\njulia> tile = @Layout (2, 2) (1, 2);\n\njulia> matrix_of_tiles = @Layout (3, 4) (4, 1);\n\njulia> print_layout(raked_product(tile, matrix_of_tiles))\n((_3, _2), (_4, _2)):((_16, _1), (_4, _2))\n       1    2    3    4    5    6    7    8\n    +----+----+----+----+----+----+----+----+\n 1  |  1 |  5 |  9 | 13 |  3 |  7 | 11 | 15 |\n    +----+----+----+----+----+----+----+----+\n 2  | 17 | 21 | 25 | 29 | 19 | 23 | 27 | 31 |\n    +----+----+----+----+----+----+----+----+\n 3  | 33 | 37 | 41 | 45 | 35 | 39 | 43 | 47 |\n    +----+----+----+----+----+----+----+----+\n 4  |  2 |  6 | 10 | 14 |  4 |  8 | 12 | 16 |\n    +----+----+----+----+----+----+----+----+\n 5  | 18 | 22 | 26 | 30 | 20 | 24 | 28 | 32 |\n    +----+----+----+----+----+----+----+----+\n 6  | 34 | 38 | 42 | 46 | 36 | 40 | 44 | 48 |\n    +----+----+----+----+----+----+----+----+\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#Division","page":"Layout","title":"Division","text":"","category":"section"},{"location":"api/layout/#MoYe.logical_divide","page":"Layout","title":"MoYe.logical_divide","text":"logical_divide(layout::Layout, tile::Tile)\n\nGather the elements of layout along all modes into blocks according to tile.\n\njulia> raked_prod = @Layout ((3, 2), (4, 2)) ((16, 1), (4, 2));\n\njulia> print_layout(raked_prod)\n((_3, _2), (_4, _2)):((_16, _1), (_4, _2))\n       1    2    3    4    5    6    7    8\n    +----+----+----+----+----+----+----+----+\n 1  |  1 |  5 |  9 | 13 |  3 |  7 | 11 | 15 |\n    +----+----+----+----+----+----+----+----+\n 2  | 17 | 21 | 25 | 29 | 19 | 23 | 27 | 31 |\n    +----+----+----+----+----+----+----+----+\n 3  | 33 | 37 | 41 | 45 | 35 | 39 | 43 | 47 |\n    +----+----+----+----+----+----+----+----+\n 4  |  2 |  6 | 10 | 14 |  4 |  8 | 12 | 16 |\n    +----+----+----+----+----+----+----+----+\n 5  | 18 | 22 | 26 | 30 | 20 | 24 | 28 | 32 |\n    +----+----+----+----+----+----+----+----+\n 6  | 34 | 38 | 42 | 46 | 36 | 40 | 44 | 48 |\n    +----+----+----+----+----+----+----+----+\n\njulia> subtile = (Layout(2, 3), Layout(2, 4)); # gather 2 elements with stride 3 along the first mode\n       # and 2 elements with stride 4 along the second mode\n\njulia> print_layout(logical_divide(raked_prod, subtile))\n(((1, 2), ((3, 1), (1, 1))), ((1, 2), ((4, 1), (1, 1)))):(((48, 1), ((_16, _1), (48, 2))), ((16, 2), ((_4, _2), (16, 4))))\n       1    2    3    4    5    6    7    8\n    +----+----+----+----+----+----+----+----+\n 1  |  1 |  3 |  5 |  7 |  9 | 11 | 13 | 15 |\n    +----+----+----+----+----+----+----+----+\n 2  |  2 |  4 |  6 |  8 | 10 | 12 | 14 | 16 |\n    +----+----+----+----+----+----+----+----+\n 3  | 17 | 19 | 21 | 23 | 25 | 27 | 29 | 31 |\n    +----+----+----+----+----+----+----+----+\n 4  | 18 | 20 | 22 | 24 | 26 | 28 | 30 | 32 |\n    +----+----+----+----+----+----+----+----+\n 5  | 33 | 35 | 37 | 39 | 41 | 43 | 45 | 47 |\n    +----+----+----+----+----+----+----+----+\n 6  | 34 | 36 | 38 | 40 | 42 | 44 | 46 | 48 |\n    +----+----+----+----+----+----+----+----+\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#MoYe.zipped_divide","page":"Layout","title":"MoYe.zipped_divide","text":"zipped_divide(layout::Layout, tile)\n\nCompute the logical division of layout by tile, then group the resulting subtiles into the first mode and the rest into the second mode.\n\njulia> raked_prod = @Layout ((3, 2), (4, 2)) ((16, 1), (4, 2));\n\njulia> print_layout(raked_prod)\n((_3, _2), (_4, _2)):((_16, _1), (_4, _2))\n       1    2    3    4    5    6    7    8\n    +----+----+----+----+----+----+----+----+\n 1  |  1 |  5 |  9 | 13 |  3 |  7 | 11 | 15 |\n    +----+----+----+----+----+----+----+----+\n 2  | 17 | 21 | 25 | 29 | 19 | 23 | 27 | 31 |\n    +----+----+----+----+----+----+----+----+\n 3  | 33 | 37 | 41 | 45 | 35 | 39 | 43 | 47 |\n    +----+----+----+----+----+----+----+----+\n 4  |  2 |  6 | 10 | 14 |  4 |  8 | 12 | 16 |\n    +----+----+----+----+----+----+----+----+\n 5  | 18 | 22 | 26 | 30 | 20 | 24 | 28 | 32 |\n    +----+----+----+----+----+----+----+----+\n 6  | 34 | 38 | 42 | 46 | 36 | 40 | 44 | 48 |\n    +----+----+----+----+----+----+----+----+\n\njulia> subtile = (@Layout(2, 3), @Layout(2, 4)); # gather 2 elements with stride 3 along the first mode and 2 elements with stride 4 along the second mode\n\njulia> print_layout(zipped_divide(raked_prod, subtile))\n((_2, _2), (_3, _4)):((_1, _2), (_16, _4))\n       1    2    3    4    5    6    7    8    9   10   11   12\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 1  |  1 | 17 | 33 |  5 | 21 | 37 |  9 | 25 | 41 | 13 | 29 | 45 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 2  |  2 | 18 | 34 |  6 | 22 | 38 | 10 | 26 | 42 | 14 | 30 | 46 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 3  |  3 | 19 | 35 |  7 | 23 | 39 | 11 | 27 | 43 | 15 | 31 | 47 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n 4  |  4 | 20 | 36 |  8 | 24 | 40 | 12 | 28 | 44 | 16 | 32 | 48 |\n    +----+----+----+----+----+----+----+----+----+----+----+----+\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#MoYe.tiled_divide","page":"Layout","title":"MoYe.tiled_divide","text":"tiled_divide(layout::Layout, tile)\n\nSimilar to zipped_divide, but upack the second mode into multiple modes.\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#Miscellaneous","page":"Layout","title":"Miscellaneous","text":"","category":"section"},{"location":"api/layout/#Base.coalesce","page":"Layout","title":"Base.coalesce","text":"coalesce(layout::Layout)\n\nCoalesce the layout by merging adjacent dimensions with stride 1.\n\nExamples\n\njulia> layout = @Layout (2, (1, 6)) (1, (6, 2))\n(_2, (_1, _6)):(_1, (_6, _2))\n\n\njulia> print(coalesce(layout))\n_12:_1\n\n\n\n\n\n","category":"function"},{"location":"api/layout/#MoYe.flatten-Tuple{Layout}","page":"Layout","title":"MoYe.flatten","text":"flatten(layout::Layout)\n\nRemove the hierarchy of the layout and make it a flat layout.\n\nExamples\n\njulia> layout = make_layout(((4, 3), 1), ((3, 1), 0))\n((4, 3), 1):((3, 1), 0)\n\n\njulia> print(flatten(layout))\n(4, 3, 1):(3, 1, 0)\n\n\n\n\n\n","category":"method"},{"location":"api/atom/#MMA/Copy-Atom","page":"MMA/Copy Atoms","title":"MMA/Copy Atom","text":"","category":"section"},{"location":"api/atom/#Index","page":"MMA/Copy Atoms","title":"Index","text":"","category":"section"},{"location":"api/atom/","page":"MMA/Copy Atoms","title":"MMA/Copy Atoms","text":"Pages = [\"atom.md\"]","category":"page"},{"location":"api/atom/#MoYe.make_tiled_mma","page":"MMA/Copy Atoms","title":"MoYe.make_tiled_mma","text":"make_tiled_mma(mma_op, atom_layout, permutations)\n\nCreate a TiledMMA object from an MMA operation, atom layout, and permutations. See also print_typst.\n\nArguments\n\nmma_op::OP: The MMA operation.\natom_layout::Layout: The layout of the atom.\npermutations::Tile: The permutations of the atom.\n\nExamples\n\njulia> tiled_mma = make_tiled_mma(MMAOP_8x8x4_F32F16F16F32_NT(), @Layout((2,2), (2,1)), (@Layout((4,4,2), (1,8,4)), _32, _4))\nTiledMMA\n  ThrLayoutVMNK: ((_4, _2), _2, _2, _1):((_1, _16), _8, _4, _0)\n  PermutationMNK: ((_4, _4, _2):(_1, _8, _4), _32, _4)\nMMAAtom\n  Thread ID: (_4, _2):(_1, _16)\n  Layout_A_TV: ((_4, _2), _4):((_8, _4), _1)\n  Layout_B_TV: ((_4, _2), _4):((_8, _4), _1)\n  Layout_C_TV: ((_2, _2, _2), (_2, _2, _2)):((_1, _16, _4), (_8, _2, _32))\n\n\n\n\n\n\n","category":"function"},{"location":"api/atom/#MoYe.make_tiled_copy","page":"MMA/Copy Atoms","title":"MoYe.make_tiled_copy","text":"make_tiled_copy(copy_atom::CopyAtom,\n                thr_layout::Layout,\n                val_layout::Layout)\n\nMake a tiled copy atom from a copy atom.\n\n\n\n\n\n","category":"function"},{"location":"api/atom/#MoYe.print_typst","page":"MMA/Copy Atoms","title":"MoYe.print_typst","text":"print_typst(::AbstractMMAAtom)\n\nPrint the layout of the A, B, and C matrices in a typst format. Go to https://typst.app and paste the output to visualize the layout.\n\nExample\n\njulia> tiled_mma = make_tiled_mma(MMAOP_8x8x4_F32F16F16F32_NT(), @Layout((2,2), (2,1)), (@Layout((4,4,2), (1,8,4)), _32, _4))\nTiledMMA\n  ThrLayoutVMNK: ((_4, _2), _2, _2, _1):((_1, _16), _8, _4, _0)\n  PermutationMNK: ((_4, _4, _2):(_1, _8, _4), _32, _4)\nMMAAtom\n  Thread ID: (_4, _2):(_1, _16)\n  Layout_A_TV: ((_4, _2), _4):((_8, _4), _1)\n  Layout_B_TV: ((_4, _2), _4):((_8, _4), _1)\n  Layout_C_TV: ((_2, _2, _2), (_2, _2, _2)):((_1, _16, _4), (_8, _2, _32))\n\n\njulia> print_typst(tiled_mma)\n\nIt will print the following image: (Image: )\n\n\n\n\n\nprint_typst(::AbstractCopyAtom)\n\nPrint the layout of the source and destination matrices in a typst format.\n\nExample\n\njulia> tiled_copy = make_tiled_copy(CopyAtom{UniversalCopy{UInt128}, Float32}(), \n                                    @Layout((32,8)), \n                                    @Layout((4,1)))\n\njulia> print_typst(tiled_copy)\n\n\n\n\n\n","category":"function"},{"location":"manual/layout/#Layout","page":"Layout","title":"Layout","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"Mathematically, a Layout represents a function that maps a logical coordinate to a 1-D index space that can be used to index into an array. It consists of a shape and a stride, wherein the shape determines the domain, and the stride establishes the mapping through an inner product. shape and stride  are both defined by (recursive) tuples of integers.","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"For example, we can construct a vector with stride 2 ","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"using MoYe\nstruct StrideVector\n   data\n   layout\nend\n\nBase.getindex(x::StrideVector, i) = x.data[x.layout(i)]\na = StrideVector(collect(1:8), Layout(4, 2))\n@show a[1] a[2] a[3] a[4];","category":"page"},{"location":"manual/layout/#Fundamentals","page":"Layout","title":"Fundamentals","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"using MoYe\nlayout_2x4 = Layout((2, (2, 2)), (4, (1, 2)))\nprint(\"Shape: \", shape(layout_2x4))\nprint(\"Stride: \", stride(layout_2x4))\nprint(\"Size: \", size(layout_2x4)) # the domain is (1,2,...,8)\nprint(\"Rank: \", rank(layout_2x4))\nprint(\"Depth: \", depth(layout_2x4))\nprint(\"Cosize: \", cosize(layout_2x4)) \nlayout_2x4 # this can be viewed as a row-major matrix","category":"page"},{"location":"manual/layout/#Compile-time-ness-of-values","page":"Layout","title":"Compile-time-ness of values","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"You can also use static integers:","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"static_layout = @Layout (2, (2, 2)) (4, (1, 2))\ntypeof(static_layout)\nsizeof(static_layout)\n","category":"page"},{"location":"manual/layout/#Different-results-from-static-Layout-vs-dynamic-Layout","page":"Layout","title":"Different results from static Layout vs dynamic Layout","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"It is expected to get results that appears to be different when the layout  is static or dynamic. For example,","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"layout = @Layout (2, (1, 6)) (1, (6, 2)) \nprint(coalesce(layout))","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"is different from","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"layout = Layout((2, (1, 6)), (1, (6, 2))) \nprint(coalesce(layout))","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"But they are mathematically equivalent. Static information allows us to simplify the result as much as possible, whereas dynamic layouts result in dynamic checking hence type  instability. ","category":"page"},{"location":"manual/layout/#Coordinate-space","page":"Layout","title":"Coordinate space","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"The coordinate space of a Layout is determined by its Shape. This coordinate space can be viewed in three different ways:","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"h-D coordinate space: Each element in this space possesses the exact hierarchical structure as defined by the Shape. Here h stands for \"hierarchical\".\n1-D coordinate space: This can be visualized as the colexicographically flattening of the coordinate space into a one-dimensional space.\nR-D coordinate space: In this space, each element has the same rank as the Shape, but each mode (top-level axis) of the Shape is colexicographically flattened into a one-dimensional space. Here R stands for the rank of the layout.","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"layout_2x4(2, (1, 2)) # h-D coordinate\nlayout_2x4(2, 3) # R-D coordinate\nlayout_2x4(6) # 1-D coordinate","category":"page"},{"location":"manual/layout/#Layout-Algebra","page":"Layout","title":"Layout Algebra","text":"","category":"section"},{"location":"manual/layout/#Concatenation","page":"Layout","title":"Concatenation","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"A layout can be expressed as the concatenation of its sublayouts.","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"layout_2x4[2] # get the second sublayout\ntuple(layout_2x4...) # splatting a layout into sublayouts\nmake_layout(layout_2x4...) # concatenating sublayouts\nfor sublayout in layout_2x4 # iterating a layout\n   @show sublayout\nend","category":"page"},{"location":"manual/layout/#Complement","page":"Layout","title":"Complement","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"Let's assume that we are dealing with a vector of 24 elements. Our goal is to partition this vector into six tiles, each consisting of four elements, following a specific pattern: gather every 4 elements at even indices to a tile.","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"This operation creates a new layout where we collect every second element until we have four elements, and then repeat this process for the rest of the vector.","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"The resulting layout would resemble:","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"       1    2    3    4    5    6\n    +----+----+----+----+----+----+\n 1  |  1 |  2 |  9 | 10 | 17 | 18 |\n    +----+----+----+----+----+----+\n 2  |  3 |  4 | 11 | 12 | 19 | 20 |\n    +----+----+----+----+----+----+\n 3  |  5 |  6 | 13 | 14 | 21 | 22 |\n    +----+----+----+----+----+----+\n 4  |  7 |  8 | 15 | 16 | 23 | 24 |\n    +----+----+----+----+----+----+","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"complement computes the first row of this new layout. ","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"print_layout(complement(@Layout(4,2), 24))","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"The layout Layout(4,2) and it complement gives us the desired new layout.","category":"page"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"print_layout(make_layout(@Layout(4, 2),complement(@Layout(4, 2), 24)))","category":"page"},{"location":"manual/layout/#Product","page":"Layout","title":"Product","text":"","category":"section"},{"location":"manual/layout/#Logical-product","page":"Layout","title":"Logical product","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"tile = @Layout((2,2), (1,2));\nprint_layout(tile)\nmatrix_of_tiles = @Layout((3,4), (4,1));\nprint_layout(matrix_of_tiles)\nprint_layout(logical_product(tile, matrix_of_tiles))","category":"page"},{"location":"manual/layout/#Blocked-product","page":"Layout","title":"Blocked product","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"print_layout(blocked_product(tile, matrix_of_tiles))","category":"page"},{"location":"manual/layout/#Raked-product","page":"Layout","title":"Raked product","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"print_layout(raked_product(tile, matrix_of_tiles))","category":"page"},{"location":"manual/layout/#Division","page":"Layout","title":"Division","text":"","category":"section"},{"location":"manual/layout/#Logical-division","page":"Layout","title":"Logical division","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"raked_prod = raked_product(tile, matrix_of_tiles);\nsubtile = (@Layout(2,3), @Layout(2,4));\nprint_layout(logical_divide(raked_prod, subtile))","category":"page"},{"location":"manual/layout/#Zipped-division","page":"Layout","title":"Zipped division","text":"","category":"section"},{"location":"manual/layout/","page":"Layout","title":"Layout","text":"print_layout(zipped_divide(raked_prod, subtile))","category":"page"},{"location":"api/copy/#Data-Movement","page":"Data Movement","title":"Data Movement","text":"","category":"section"},{"location":"api/copy/#Index","page":"Data Movement","title":"Index","text":"","category":"section"},{"location":"api/copy/","page":"Data Movement","title":"Data Movement","text":"Pages = [\"copy.md\"]","category":"page"},{"location":"api/copy/#Base.copyto!-Tuple{MoYeArray, MoYeArray}","page":"Data Movement","title":"Base.copyto!","text":"copyto!(dest::MoYeArray, src::MoYeArray)\n\nCopy the contents of src to dest. The function automatically carries out potential vectorization. In particular, while transferring data from global memory to shared memory, it automatically initiates asynchronous copying, if your device supports so.\n\n\n\n\n\n","category":"method"},{"location":"api/copy/#Base.copyto!-Tuple{MoYe.AbstractLdMatrix, MoYeArray, MoYeArray}","page":"Data Movement","title":"Base.copyto!","text":"copyto!(ldmatrix::AbstractLdMatrix, dest::MoYeArray{UInt32}, src::MoYeArray{UInt128})\n\nLoad data from shared memory to registers. The available AbstractLdMatrixs are:\n\n# Type => LLVM intrinsic\n\"LDSM_U32x1_N\" => \"llvm.nvvm.ldmatrix.sync.aligned.m8n8.x1.b16\"\n\"LDSM_U32x2_N\" => \"llvm.nvvm.ldmatrix.sync.aligned.m8n8.x2.b16\"\n\"LDSM_U32x4_N\" => \"llvm.nvvm.ldmatrix.sync.aligned.m8n8.x4.b16\"\n\"LDSM_U16x2_T\" => \"llvm.nvvm.ldmatrix.sync.aligned.m8n8.x1.trans.b16\"\n\"LDSM_U16x4_T\" => \"llvm.nvvm.ldmatrix.sync.aligned.m8n8.x2.trans.b16\"\n\"LDSM_U16x8_T\" => \"llvm.nvvm.ldmatrix.sync.aligned.m8n8.x4.trans.b16\"\n\nYou can inspect the number and the type of  registers used per thread by\n\njulia> LDSM_U32x4_N()\nLDSM_U32x4_N()\n\njulia> ans.DRegisters\nRegisters{UInt32, 4}\n\n\n\n\n\n","category":"method"},{"location":"api/copy/#MoYe.cp_async_wait","page":"Data Movement","title":"MoYe.cp_async_wait","text":"cp_async_wait(N::Int32)\ncp_async_wait()\n\ncp_async_wait(N) is equivalent to cp.async.wait.group(N) and cp_async_wait() is equivalent to cp.async.wait.all in CUDA.\n\n\n\n\n\n","category":"function"},{"location":"api/copy/#MoYe.cp_async_commit","page":"Data Movement","title":"MoYe.cp_async_commit","text":"cp_async_commit()\n\ncp.async.commit.group.\n\n\n\n\n\n","category":"function"},{"location":"#MoYe","page":"Home","title":"MoYe","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MoYe.","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> data = [i for i in 1:48];\n\njulia> a = MoYeArray(data, @Layout((6,8)))\n6×8 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{6}, Static.StaticInt{8}}, Tuple{Static.StaticInt{1}, Static.StaticInt{6}}}} with indices _1:_6×_1:_8:\n 1   7  13  19  25  31  37  43\n 2   8  14  20  26  32  38  44\n 3   9  15  21  27  33  39  45\n 4  10  16  22  28  34  40  46\n 5  11  17  23  29  35  41  47\n 6  12  18  24  30  36  42  48\n\njulia> subtile_a = @tile a (_3, _4) (1, 2)\n3×4 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{3}, Static.StaticInt{4}}, Tuple{Static.StaticInt{1}, Static.StaticInt{6}}}} with indices _1:_3×_1:_4:\n 25  31  37  43\n 26  32  38  44\n 27  33  39  45\n\njulia> workitems_a = @parallelize subtile_a (_3, _2) (1, 1)\n1×2 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{1}, Static.StaticInt{2}}, Tuple{Static.StaticInt{0}, Static.StaticInt{12}}}} with indices _1:_1×_1:_2:\n 25  37\n\njulia> for i in eachindex(workitems_a)\n                  workitems_a[i] = 0\n              end\n\njulia> a\n6×8 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{6}, Static.StaticInt{8}}, Tuple{Static.StaticInt{1}, Static.StaticInt{6}}}} with indices _1:_6×_1:_8:\n 1   7  13  19   0  31   0  43\n 2   8  14  20  26  32  38  44\n 3   9  15  21  27  33  39  45\n 4  10  16  22  28  34  40  46\n 5  11  17  23  29  35  41  47\n 6  12  18  24  30  36  42  48\n\njulia> @tile subtile_a (_3, _1) (1, 2)\n3×1 MoYeArray{Int64, 2, ViewEngine{Int64, Ptr{Int64}}, Layout{2, Tuple{Static.StaticInt{3}, Static.StaticInt{1}}, Tuple{Static.StaticInt{1}, Static.StaticInt{0}}}} with indices _1:_3×_1:_1:\n 31\n 32\n 33","category":"page"},{"location":"#Tile-Iterator","page":"Home","title":"Tile Iterator","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using MoYe \ndata = collect(1:36);\nA = MoYeArray(data, @Layout((4,9)))\ntiled_A = zipped_divide(A, (@Layout(2), @Layout(3))) # 2 × 3 tile\nfor i in axes(tiled_A, 2)\n    @show view(tiled_A, :, i)\nend","category":"page"},{"location":"manual/pipeline/#Overlap-global-to-shared-copies-with-mma-compute","page":"Pipeline","title":"Overlap global-to-shared copies with mma compute","text":"","category":"section"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"We can overlap global-to-shared memory copies with mma compute.","category":"page"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"(Image: )","category":"page"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"To do this we will explicitly load data from shared memory to registers for the mma computation and submit a new load from global memory to shared memory for the next tile before compute.","category":"page"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"function matmul_kernel(A, sA_layout, copy_A,\n                       B, sB_layout, copy_B,\n                       C, mma_C)\n    sA = MoYeSharedArray(eltype(A), sA_layout)\n    sB = MoYeSharedArray(eltype(B), sB_layout)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # copy partition\n    thr_copy_a = get_slice(copy_A, threadIdx().x)      \n    tAgA = partition_S(thr_copy_a, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(thr_copy_a, sA)                 # (CPY, CPY_M, CPY_K)\n\n    thr_copy_b = get_slice(copy_B, threadIdx().x)\n    tBgB = partition_S(thr_copy_b, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(thr_copy_b, sB)                 # (CPY, CPY_N, CPY_K)\n\n    # Copy gmem to smem for k_tile=1\n    copyto!(copy_A, tAsA, view(tAgA, :, :, :, _1))\n    copyto!(copy_B, tBsB, view(tBgB, :, :, :, _1))\n\n    # mma partition\n    thr_mma = get_slice(mma_C, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                    # (MMA, MMA_M, MMA_K)\n    tCsB = partition_B(thr_mma, sB)                    # (MMA, MMA_M, MMA_K)\n    tCgC = partition_C(thr_mma, gC)                    # (MMA, MMA_M, MMA_N)\n\n    # mma registers\n    tCrA = make_fragment_A(thr_mma, tCsA)                # (MMA, MMA_M, MMA_K)\n    tCrB = make_fragment_B(thr_mma, tCsB)                # (MMA, MMA_N, MMA_K)\n    tCrC = make_fragment_C(thr_mma, tCgC)                # (MMA, MMA_M, MMA_N)\n    zeros!(tCrC)\n\n    k_max = size(tAgA, 4)\n    for k in 1:k_max\n        cp_async_wait()\n        sync_threads()\n\n        # copy from smem to rmem\n        copyto!(tCrA, tCsA)\n        copyto!(tCrB, tCsB)\n        sync_threads()\n\n        if k < k_max\n            copyto!(copy_A, tAsA, view(tAgA, :, :, :, k+1))\n            copyto!(copy_B, tBsB, view(tBgB, :, :, :, k+1))\n        end\n\n        @gc_preserve gemm!(mma_C, tCrC, tCrA, tCrB, tCrC)\n    end\n\n    copyto!(tCgC, tCrC)\n    return nothing\nend","category":"page"},{"location":"manual/pipeline/#Double-buffer","page":"Pipeline","title":"Double buffer","text":"","category":"section"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"We can also overlap shared-to-registers memory copies with mma compute.","category":"page"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"To do this we will need to allocate two shared memory buffers, one for the current compute and one for the next tile. We prefetch the next tile from global memory to shared memory asynchronously.","category":"page"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"(Image: matmuil)","category":"page"},{"location":"manual/pipeline/","page":"Pipeline","title":"Pipeline","text":"@views function matmul_kernel(A, sA_layout, copy_A,\n                              B, sB_layout, copy_B,\n                              C, mma_C)\n    sA = MoYeSharedArray(eltype(A), sA_layout)        # (bM, bK, 2)\n    sB = MoYeSharedArray(eltype(B), sB_layout)        # (bN, bK, 2)\n\n    mA = MoYeArray(A)\n    mB = MoYeArray(B)\n    mC = MoYeArray(C)\n\n    bM = size(sA_layout, 1)\n    bN = size(sB_layout, 1)\n    bK = size(sB_layout, 2)\n\n    gA = @tile mA (bM, bK) (blockIdx().x, :)\n    gB = @tile mB (bN, bK) (blockIdx().y, :)\n    gC = @tile mC (bM, bN) (blockIdx().x, blockIdx().y)\n\n    # copy partition\n    thr_copy_a = get_slice(copy_A, threadIdx().x)      \n    tAgA = partition_S(thr_copy_a, gA)                 # (CPY, CPY_M, CPY_K, k)\n    tAsA = partition_D(thr_copy_a, sA)                 # (CPY, CPY_M, CPY_K, 2)\n\n    thr_copy_b = get_slice(copy_B, threadIdx().x)\n    tBgB = partition_S(thr_copy_b, gB)                 # (CPY, CPY_N, CPY_K, k)\n    tBsB = partition_D(thr_copy_b, sB)                 # (CPY, CPY_N, CPY_K, 2)\n\n    # Copy gmem to smem for k_tile=1\n    copyto!(copy_A, tAsA[:, :, :, 1], tAgA[:, :, :, _1])\n    copyto!(copy_B, tBsB[:, :, :, 1], tBgB[:, :, :, _1])\n\n    # mma partition\n    thr_mma = get_slice(mma_C, threadIdx().x)\n    tCsA = partition_A(thr_mma, sA)                    # (MMA, MMA_M, MMA_K, 2)\n    tCsB = partition_B(thr_mma, sB)                    # (MMA, MMA_M, MMA_K, 2)\n    tCgC = partition_C(thr_mma, gC)                    # (MMA, MMA_M, MMA_N)\n\n    # mma registers\n    tCrA = make_fragment_A(thr_mma, tCsA[:, :, :, _1])    # (MMA, MMA_M, MMA_K)\n    tCrB = make_fragment_B(thr_mma, tCsB[:, :, :, _1])    # (MMA, MMA_N, MMA_K)\n    tCrC = make_fragment_C(thr_mma, tCgC)                 # (MMA, MMA_M, MMA_N)\n    zeros!(tCrC)\n\n    cp_async_wait()\n    sync_threads()\n\n    # Copy smem to rmem for k_block=1\n    smem_read = 1\n    smem_write = 2\n    tCsA_p = view(tCsA, :, :, :, smem_read)\n    tCsB_p = view(tCsB, :, :, :, smem_read)\n    copyto!(tCrA[:, :, 1], tCsA_p[:, :, _1])\n    copyto!(tCrB[:, :, 1], tCsB_p[:, :, _1])\n\n    k_tile_max = size(tAgA, 4)\n    k_block_max = static_size(tCrA, 3)\n    for k_tile in 1:k_tile_max\n        @loopinfo unroll for k_block in _1:k_block_max\n            k_block_next = k_block + 1 \n            if k_block == k_block_max\n                cp_async_wait()\n                sync_threads()\n                tCsA_p = view(tCsA, :, :, :, smem_read)\n                tCsB_p = view(tCsB, :, :, :, smem_read)\n                k_block_next = 1\n            end\n            \n            copyto!(tCrA[:, :, k_block_next], tCsA_p[:, :, k_block_next])\n            copyto!(tCrB[:, :, k_block_next], tCsB_p[:, :, k_block_next])   \n\n            if k_block == _1 && k_tile<k_tile_max\n                copyto!(copy_A, tAsA[:, :, :, smem_write], tAgA[:, :, :, k_tile+1])\n                copyto!(copy_B, tBsB[:, :, :, smem_write], tBgB[:, :, :, k_tile+1])\n                smem_read, smem_write = smem_write, smem_read\n            end\n            \n            @gc_preserve gemm!(mma_C, tCrC, tCrA[:, :, k_block], tCrB[:, :, k_block], tCrC)\n        end\n    end\n\n    copyto!(tCgC, tCrC)\n    return nothing\nend\n\nfunction matmul(A, B, C)\n    bM = _128\n    bN = _128\n    bK = _8\n    \n    sA_layout = make_layout((bM, bK, _2), (_1, bM + _2, (bM + _2) * bK))\n    sB_layout = make_layout((bN, bK, _2), (_1, bN + _2, (bN + _2) * bK))\n\n    TA = eltype(A)\n    TB = eltype(B)\n    TC = eltype(C)\n\t\n    copy_A = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{Float64}, TA}(),\n                             @Layout((32, 8)),\n                             @Layout((2, 1)))\n    copy_B = make_tiled_copy(CopyAtom{CPOP_ASYNC_CACHEALWAYS{Float64}, TB}(),\n                             @Layout((32, 8)),\n                             @Layout((2, 1)))\n\n    mma_C = make_tiled_mma(UniversalFMA{TA,TB, TC}(), # MMA operation\n                           @Layout((32, 8)))          # Atom layout\n\n    threads = Int(size(mma_C))\n    blocks = (cld(size(A, 1), bM), cld(size(B, 1), bN))\n\n    @cuda threads=threads blocks=blocks matmul_kernel(A, sA_layout, copy_A,\n                                                      B, sB_layout, copy_B,\n                                                      C, mma_C)\nend\n\nfunction test()\n    A =  CUDA.randn(Float32, 2048, 256)\n    B =  CUDA.randn(Float32, 2048, 256)\n    C =  CUDA.randn(Float32, 2048, 2048)\n    matmul(A, B, C)\n    CUDA.synchronize()\n    @test C == A * B'\n    CUDA.unsafe_free!(A)\n    CUDA.unsafe_free!(B)\n    CUDA.unsafe_free!(C)\nend\n\ntest()","category":"page"}]
}
